{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d0f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib.colors import rgb2hex\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42bf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../cpunks/data\"\n",
    "image_dir = \"../cpunks/images/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf36ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punk(id):\n",
    "    '''\n",
    "       Returns a ndarray with loaded image\n",
    "    ''' \n",
    "    return mpimg.imread(f'''{image_dir}/punk{\"%04d\" % id}.png''')\n",
    "\n",
    "def unique_color_alpha_vectors(image_array):\n",
    "    # Reshape the image array to a 2D array where each row is a color-alpha vector\n",
    "    reshaped_array = image_array.reshape(-1, image_array.shape[2])\n",
    "\n",
    "    # Find unique rows (color-alpha vectors) in the reshaped array\n",
    "    unique_vectors = np.unique(reshaped_array, axis=0)\n",
    "\n",
    "    return unique_vectors\n",
    "\n",
    "def find_unique_vectors_across_arrays(arrays):\n",
    "    # Concatenate all arrays along the first two dimensions\n",
    "    concatenated_array = np.concatenate([arr.reshape(-1, 4) for arr in arrays], axis=0)\n",
    "\n",
    "    # Find unique rows (color vectors) in the concatenated array\n",
    "    unique_vectors = np.unique(concatenated_array, axis=0)\n",
    "\n",
    "    return unique_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36b887",
   "metadata": {},
   "source": [
    "## load all 10k punks and find set of unique colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864aacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [get_punk(i) for i in range(0,10000)]\n",
    "all_colors = find_unique_vectors_across_arrays(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8451ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ColorOneHotMapper:\n",
    "    def __init__(self, unique_colors):\n",
    "        self.color_to_one_hot = {}\n",
    "        self.one_hot_to_color = {}\n",
    "\n",
    "        for idx, color in enumerate(unique_colors):\n",
    "            one_hot = np.zeros(len(unique_colors))\n",
    "            one_hot[idx] = 1\n",
    "\n",
    "            self.color_to_one_hot[tuple(color)] = one_hot\n",
    "            self.one_hot_to_color[tuple(one_hot)] = color\n",
    "\n",
    "    def get_one_hot(self, color):\n",
    "        color_tuple = tuple(color)\n",
    "        return self.color_to_one_hot.get(color_tuple, \n",
    "                                         \"Color not found\")\n",
    "\n",
    "    def get_color(self, one_hot):\n",
    "        one_hot_tuple = tuple(one_hot)\n",
    "        return self.one_hot_to_color.get(one_hot_tuple, \n",
    "                                         \"One-hot vector not found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7ccbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: [0.        0.2509804 1.        1.       ]\n",
      "One-hot encoding: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Retrieved color: (4,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "unique_colors = all_colors\n",
    "mapper = ColorOneHotMapper(unique_colors)\n",
    "\n",
    "# Test with a color\n",
    "test_color = unique_colors[2]\n",
    "test_one_hot = mapper.get_one_hot(test_color)\n",
    "print(\"Color:\", test_color)\n",
    "print(\"One-hot encoding:\", test_one_hot)\n",
    "\n",
    "# Test with a one-hot vector\n",
    "retrieved_color = mapper.get_color(test_one_hot)\n",
    "print(\"Retrieved color:\", retrieved_color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb1ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_image_to_one_hot(image, mapper):\n",
    "    # Initialize an empty array for the one-hot encoded image\n",
    "    one_hot_encoded_image = np.zeros((image.shape[0], image.shape[1], len(mapper.color_to_one_hot)))\n",
    "\n",
    "    # Iterate over each pixel and convert to one-hot encoding\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            color = image[i, j]\n",
    "            one_hot = mapper.get_one_hot(color)\n",
    "            if isinstance(one_hot, str):  # Check if color not found\n",
    "                continue  # or handle the error as needed\n",
    "            one_hot_index = np.argmax(one_hot)\n",
    "            one_hot_encoded_image[i, j, one_hot_index] = 1\n",
    "\n",
    "    return one_hot_encoded_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94b5080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "image = get_punk(0)\n",
    "# Convert to one-hot encoded image\n",
    "one_hot_encoded_image = convert_image_to_one_hot(image, mapper)\n",
    "\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3f7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class OneHotEncodedImageDataset(Dataset):\n",
    "    def __init__(self, directory, mapper):\n",
    "        self.directory = directory\n",
    "        self.mapper = mapper\n",
    "        self.image_files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        image = mpimg.imread(image_path)\n",
    "        # Assuming get_image returns a numpy array of shape (24, 24, 4)\n",
    "        one_hot_encoded_image = convert_image_to_one_hot(image, self.mapper)\n",
    "        return torch.tensor(one_hot_encoded_image, dtype=torch.float32)\n",
    "\n",
    "def prepare_dataset(directory, mapper):\n",
    "    return OneHotEncodedImageDataset(directory, mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35fef1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_colors = all_colors\n",
    "mapper = ColorOneHotMapper(unique_colors)\n",
    "dataset_directory = image_dir  \n",
    "\n",
    "dataset = prepare_dataset(dataset_directory, mapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9218f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.OneHotEncodedImageDataset, 10000, torch.Size([24, 24, 222]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(dataset),\n",
    " len(dataset),\n",
    " dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da5508",
   "metadata": {},
   "source": [
    "### prepare a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2366d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'dataset' is your instance of OneHotEncodedImageDataset\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "shuffle = True  # Set to True to shuffle the dataset at every epoch\n",
    "num_workers = 4  # Adjust based on your system's capabilities\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b30d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: torch.Size([32, 24, 24, 222])\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data_loader' is your DataLoader instance\n",
    "for batch in dataloader:\n",
    "    # Assuming your batch contains input data and possibly labels/targets\n",
    "    inputs = batch  # or batch['input'] depending on how your dataset returns a batch\n",
    "    print(\"Shape of input:\", inputs.shape)\n",
    "    break  # We break the loop as we only want to check the shape of one batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881a7ad",
   "metadata": {},
   "source": [
    "### Define the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b434787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class OneHotAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(24*24*222, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 24*24*222),\n",
    "            nn.Sigmoid()  # Use sigmoid to ensure output is between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(x.size(0), 24, 24, 222)  # Reshape back to the original shape\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b9f3f",
   "metadata": {},
   "source": [
    "### setup the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb41de",
   "metadata": {},
   "source": [
    "PyTorch's nn.Conv2d expects input in the format (batch_size, channels, height, width), but our dataloader used the format (batch_size, height, width, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c733c096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0014385608956217766\n",
      "Epoch [2/5], Loss: 0.0011465066345408559\n",
      "Epoch [3/5], Loss: 0.0010496065951883793\n",
      "Epoch [4/5], Loss: 0.0010131453163921833\n",
      "Epoch [5/5], Loss: 0.000994367990642786\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(model, dataloader, epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data in dataloader:\n",
    "            # Permute the dimensions of the input to match (batch_size, channels, height, width)\n",
    "            #inputs = data.permute(0, 3, 1, 2)  # Change from [32, 24, 24, 222] to [32, 222, 24, 24]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Create the model instance\n",
    "autoencoder = OneHotAutoencoder()\n",
    "\n",
    "# Assuming you have a DataLoader named 'dataloader'\n",
    "train_autoencoder(autoencoder, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5326f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24, 222)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector = torch.randn(1, 64)  # Generate a random vector (1, 64)\n",
    "\n",
    "autoencoder.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    decoded_vector = autoencoder.decoder(latent_vector)\n",
    "    decoded_image = decoded_vector.view(-1, 24, 24, 222)  # Reshape to (1, 24, 24, 222)\n",
    "\n",
    "decoded_image_np = decoded_image.squeeze().numpy()  # Convert to numpy array\n",
    "decoded_image_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fff0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_to_rgb(decoded_one_hot, mapper):\n",
    "    # Choose the color with the highest probability for each pixel\n",
    "    color_indices = np.argmax(decoded_one_hot, axis=-1)\n",
    "\n",
    "    # Initialize an empty array for the RGB image\n",
    "    rgb_image = np.zeros((color_indices.shape[0], color_indices.shape[1], 4))\n",
    "\n",
    "    # Map each index back to an RGB color\n",
    "    for i in range(color_indices.shape[0]):\n",
    "        for j in range(color_indices.shape[1]):\n",
    "            one_hot_vector = get_one_hot(color_indices[i, j], 222)\n",
    "            rgb_image[i, j] = mapper.get_color(one_hot_vector)\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b9fbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(index, length):\n",
    "    # Create a zero vector of the specified length\n",
    "    one_hot_vector = np.zeros(length)\n",
    "    \n",
    "    # Set the value at the index to 1\n",
    "    one_hot_vector[index] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "302278fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = decode_to_rgb(decoded_image_np, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a187139e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
