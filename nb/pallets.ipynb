{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d0f047",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42bf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it possible to import `pallets` from parent dir\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    ")\n",
    "from pallets import images, datasets, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36b887",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "1. Gather every unique color from the 10,000 punks and generate a one hot representation for each.\n",
    "\n",
    "2. Instantiate a mapper to go from each color to its one hot representation and back.\n",
    "\n",
    "3. Create dataloader for punks that represents its color data as one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94b5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (24, 24, 4)\n",
      "Image colors:\n",
      " [[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.        ]\n",
      " [0.3137255  0.4862745  0.2        1.        ]\n",
      " [0.3647059  0.54509807 0.2627451  1.        ]\n",
      " [0.37254903 0.11372549 0.03529412 1.        ]\n",
      " [0.68235296 0.54509807 0.38039216 1.        ]\n",
      " [1.         0.9647059  0.5568628  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Unique colors for one punk\n",
    "\n",
    "image = images.get_punk(0)\n",
    "print(\"Image shape:\", image.shape)\n",
    "\n",
    "colors = images.one_image_colors(image)\n",
    "print(\"Image colors:\\n\", colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864aacb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 222 colors\n"
     ]
    }
   ],
   "source": [
    "# Unique colors for all punks\n",
    "\n",
    "all_colors = images.get_punk_colors()\n",
    "print(f\"Found {len(all_colors)} colors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d09b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image one hot:\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Mapper from color to one hot\n",
    "\n",
    "mapper = datasets.ColorOneHotMapper(all_colors)\n",
    "one_hot_encoded_image = datasets.convert_image_to_one_hot(image, mapper)\n",
    "print(\"Image one hot:\\n\", one_hot_encoded_image[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7ccbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color:\n",
      " [0.        0.2509804 1.        1.       ]\n",
      "One-hot encoding:\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Retrieved color:\n",
      " [0.        0.2509804 1.        1.       ]\n"
     ]
    }
   ],
   "source": [
    "# Start with a color\n",
    "test_color = all_colors[2]\n",
    "print(\"Color:\\n\", test_color)\n",
    "\n",
    "# Convert color to one hot\n",
    "test_one_hot = mapper.get_one_hot(test_color)\n",
    "print(\"One-hot encoding:\\n\", test_one_hot)\n",
    "\n",
    "# Convert one hat back to original color\n",
    "retrieved_color = mapper.get_color(test_one_hot)\n",
    "print(\"Retrieved color:\\n\", retrieved_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fef1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pallets.datasets.OneHotEncodedImageDataset, 10000, torch.Size([24, 24, 222]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punks Dataset\n",
    "\n",
    "dataset = datasets.OneHotEncodedImageDataset(images.CPUNKS_IMAGE_DIR, mapper, test_size=2000)\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(dataset.train_idx)\n",
    "# test_sampler = SubsetRandomSampler(dataset.test_idx)\n",
    "(type(dataset), len(dataset), dataset[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2366d73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 24, 24, 222])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punks DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    "#     sampler=train_sampler\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    "#     sampler=train_sampler\n",
    "# )\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    ")\n",
    "\n",
    "test_punk = next(iter(dataloader))\n",
    "test_punk.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b9f3f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c733c096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0015208707191050053\n",
      "Epoch [2/5], Loss: 0.0010436027077957988\n",
      "Epoch [3/5], Loss: 0.0009153225109912455\n",
      "Epoch [4/5], Loss: 0.0008805958786979318\n",
      "Epoch [5/5], Loss: 0.0006662878440693021\n"
     ]
    }
   ],
   "source": [
    "# One Hot\n",
    "autoencoder = models.OneHotAutoencoder()\n",
    "models.train_onehot(autoencoder, dataloader)\n",
    "\n",
    "# # Simple Conv\n",
    "# autoencoder = models.SimpleConvAutoencoder()\n",
    "# models.train_simple_conv(autoencoder, dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4cdbf",
   "metadata": {},
   "source": [
    "# Model Output to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf1a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 24, 222])\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct random data for test\n",
    "\n",
    "# One Hot\n",
    "if isinstance(autoencoder, models.OneHotAutoencoder):\n",
    "    example_input = torch.rand(32, 24*24*222)\n",
    "    reconstructed = autoencoder(example_input)\n",
    "    print(reconstructed.shape)\n",
    "\n",
    "# Simple Conv\n",
    "else:\n",
    "    example_input = torch.rand(32, 222, 28, 28)\n",
    "    reconstructed = autoencoder(example_input)\n",
    "    print(reconstructed.shape)  # Should be torch.Size([32, 222, 28, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0d2833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1338299e-18, 1.8741471e-17, 2.4810321e-18, 5.5339195e-21,\n",
       "       8.8515905e-19, 7.9717396e-19, 4.2279645e-08, 2.0188399e-21,\n",
       "       5.5220710e-20, 5.3137335e-17, 1.4391381e-22, 1.5680697e-28,\n",
       "       1.3898229e-18, 1.1260277e-19, 4.3473527e-21, 9.6878572e-24,\n",
       "       1.5823628e-12, 4.3565405e-14, 9.0437682e-15, 2.5792071e-17,\n",
       "       1.7835960e-24, 1.3420088e-13, 2.0429958e-21, 3.7997026e-15],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array = reconstructed.detach().cpu().numpy()\n",
    "\n",
    "# Select the image at the specified index\n",
    "one_hot_image = img_array[0]\n",
    "one_hot_image[:,12,12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be7562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot\n",
    "if isinstance(autoencoder, models.OneHotAutoencoder):\n",
    "    s = datasets.decode_to_rgb(one_hot_image, mapper)\n",
    "\n",
    "# Simple Conv\n",
    "else:\n",
    "    reordered_array = np.transpose(one_hot_image, (1, 2, 0))\n",
    "    s = datasets.decode_to_rgb(reordered_array, mapper)\n",
    "\n",
    "s.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7baa76b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d1d79",
   "metadata": {},
   "source": [
    "# Decoder to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741caa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot\n",
    "if isinstance(autoencoder, models.OneHotAutoencoder):\n",
    "    latent_vector = torch.randn(1, 64)\n",
    "\n",
    "# Simple Conv\n",
    "else:\n",
    "    latent_vector = torch.randn(32, 6, 6)\n",
    "\n",
    "\n",
    "autoencoder.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad(): \n",
    "    decoded_vector = autoencoder.decoder(latent_vector)\n",
    "    decoded_image = decoded_vector.view(-1, 24, 24, 222)  # Reshape to (1, 24, 24, 222)\n",
    "\n",
    "decoded_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_image_np = decoded_image.squeeze().numpy()  # Convert to numpy array\n",
    "decoded_image_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774511a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = datasets.decode_to_rgb(decoded_image_np, mapper)\n",
    "new_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27391853",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(new_image)\n",
    "plt.axis('off')  # Turn off axis numbers\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
